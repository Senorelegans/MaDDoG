
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/30_MultiSwitchpoint.ipynb
# %matplotlib inline

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
plt.style.use('default')
from matplotlib.patches import Rectangle
import os.path
from os import path


import scipy.stats as stats
from scipy.stats import linregress
from nb_20_Dogcatcher import *


from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}

# import os
# import tensorflow as tf
# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'


### Make a conda environment for this with python 3.8
#conda create --name tfp python=3.8
# conda activate tfp
#pip3 install tensorflow--cpu==2.4.1
#pip3 install tensorflow_probability==0.12.1
# pip install -U scikit-learn
# pip install seaborn
# pip install numpy
# pip install pandas
# conda install -c conda-forge nb_conda_kernels
# conda install ipykernel

import tensorflow as tf
import tensorflow.compat.v2 as tf
# print(tf.__version__)
tf.enable_v2_behavior()
import tensorflow_probability as tfp
from tensorflow_probability import distributions as tfd


# %matplotlib inline
import scipy.stats
#https://www.tensorflow.org/probability/examples/Multiple_changepoint_detection_and_Bayesian_model_selection

class runSwitchpoint():
    def __init__(self, fname, row, convolution_window,iterations=100, max_num_states=6, max_neg_loss_thresh=0.05, DOG_ALL="DOG",local_meta="local", df_train=pd.DataFrame(),Model_Selection_percent_dist_from_most_complex="NA",bayes_factor_thresh="NA"):
        self.local_meta = local_meta
        self.strand = row["strand"]
        self.name = row["name"]
        self.fname = fname
        self.chr = row["chr"]
        self.df_train = df_train
        self.DOG_ALL = DOG_ALL
        self.iterations = iterations
        self.convolution_window = convolution_window
        self.max_num_states = max_num_states
        self.max_neg_loss_thresh = max_neg_loss_thresh
        self.best_model = max_num_states
        self.window = 50
        self.Model_Selection_percent_dist_from_most_complex = Model_Selection_percent_dist_from_most_complex
        self.bayes_factor_thresh = bayes_factor_thresh
        if self.DOG_ALL == "ALL":
            self.region = row["region"]
            self.region_start = row["region_start"]
            self.region_end = row["region_end"]
            self.region_length = row["region_end"] - row["region_start"]
            self.region_length_window = self.region_length / self.window

        s = row[f"{DOG_ALL}_reads_{local_meta}"]
        if type(s) == str:
            s = eval(s)

        self.df = pd.DataFrame(s)
        if self.strand == "-":
            self.df.sort_values(by=["start"], inplace=True, ascending=False)
#         self.window = self.df["end"].iloc[1] - self.df["start"].iloc[1]
        self.df = self.df.reset_index(drop=True)
        self.df["window"] = self.df.index + 1
        self.df["fname"] = self.fname
        self.df["strand"] = self.strand
#         print(self.df)

        count = self.df["count"].to_numpy()
        def moving_average(x, w):
            return np.convolve(x, np.ones(w), 'valid') / w
        if self.convolution_window < len(self.df):
            self.count = moving_average(x=count,w=self.convolution_window)
        else:
            self.count = count
        self.n = len(self.count)
        self.countmax = self.count.max()
        self.title = f"{self.fname} | {self.name} | strand:{self.strand} | {self.DOG_ALL} | {self.local_meta}"

#         self.Histogram()
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)
        self.MultiState()



    def MultiState(self):
        plt.tight_layout()
        observed_counts = self.count.astype(np.float32)

        max_num_states = self.max_num_states

        def build_latent_state(num_states, max_num_states, daily_change_prob=0.05):
          # Give probability exp(-100) ~= 0 to states outside of the current model.
          initial_state_logits = -100. * np.ones([max_num_states], dtype=np.float32)
          initial_state_logits[:num_states] = 0.

          # Build a transition matrix that transitions only within the current
          # `num_states` states.
          transition_probs = np.eye(max_num_states, dtype=np.float32)
          if num_states > 1:
            transition_probs[:num_states, :num_states] = (
                daily_change_prob / (num_states-1))
            np.fill_diagonal(transition_probs[:num_states, :num_states],
                             1-daily_change_prob)
          return initial_state_logits, transition_probs

        # For each candidate model, build the initial state prior and transition matrix.
        batch_initial_state_logits = []
        batch_transition_probs = []
        for num_states in range(1, max_num_states+1):
          initial_state_logits, transition_probs = build_latent_state(
              num_states=num_states,
              max_num_states=max_num_states)
          batch_initial_state_logits.append(initial_state_logits)
          batch_transition_probs.append(transition_probs)

        batch_initial_state_logits = np.array(batch_initial_state_logits)
        batch_transition_probs = np.array(batch_transition_probs)

        trainable_log_rates = tf.Variable(
            (np.log(np.mean(observed_counts)) *
             np.ones([batch_initial_state_logits.shape[0], max_num_states]) +
             tf.random.normal([1, max_num_states])),
             name='log_rates')

        hmm = tfd.HiddenMarkovModel(
          initial_distribution=tfd.Categorical(
              logits=batch_initial_state_logits),
          transition_distribution=tfd.Categorical(probs=batch_transition_probs),
          observation_distribution=tfd.Poisson(log_rate=trainable_log_rates),
          num_steps=len(observed_counts))


        rate_prior = tfd.LogNormal(5, 5)

        def log_prob():
          prior_lps = rate_prior.log_prob(tf.math.exp(trainable_log_rates))
          prior_lp = tf.stack(
              [tf.reduce_sum(prior_lps[i, :i+1]) for i in range(max_num_states)])
          return prior_lp + hmm.log_prob(observed_counts)

        @tf.function(autograph=False)
        def train_op():
          with tf.GradientTape() as tape:
            neg_log_prob = -log_prob()
          grads = tape.gradient(neg_log_prob, [trainable_log_rates])[0]
          self.optimizer.apply_gradients([(grads, trainable_log_rates)])
          return neg_log_prob, tf.math.exp(trainable_log_rates)

        for step in range(self.iterations):
          loss, rates =  [t.numpy() for t in train_op()]
#           if step % 20 == 0:
#             print(f"step {step}: loss {loss}")

        num_states = np.arange(1, max_num_states+1)
        plt.plot(num_states, -loss)
        plt.ylim([-5000, -10])
        plt.ylabel("log marginal likelihood $\\tilde{p}(x)$")
        plt.xlabel("number of latent states")
        plt.title("Model selection on latent states")
#         outpath = f"{self.OutFolder}/Plots/MultiState/{self.name}"
        plt.savefig(f"{self.name}_ModelSelection_{self.fname}.png", bbox_inches='tight')
        plt.close()

        df_ms = pd.DataFrame.from_dict({"state_model":num_states,"neg_loss":-loss})#,"slope":slope})
        df_m = df_ms.copy()
        df_m = df_m.drop_duplicates(subset=["state_model"])

        best_neg_loss = float(df_m["neg_loss"].iloc[0])
        best_state_model = float(df_m["state_model"].iloc[0])

        for i in range(1,len(df_m)):
            curr_loss = df_m["neg_loss"].iloc[i]
            bayes_factor =  best_neg_loss / curr_loss
            state_model = df_m["state_model"].iloc[i]
            if bayes_factor > self.bayes_factor_thresh:
                best_neg_loss = curr_loss
                best_state_model = state_model
        self.best_model = best_state_model


        posterior_probs = hmm.posterior_marginals(
            observed_counts).probs_parameter().numpy()
        most_probable_states = np.argmax(posterior_probs, axis=-1)


        d = {}
        states = []
        for i, learned_model_rates in enumerate(rates):
          state = i + 1
          d[state] = learned_model_rates[most_probable_states[i]]
          states.append(state)

        # Write out csv of switchpoints
        df_s = pd.DataFrame.from_dict(d)

        df_s = df_s.reset_index(drop=True)
        df_s["window"] = df_s.index + 1
        df_switch = pd.DataFrame()

        for state in states:
            df_s[f"State_{state}_switchpoint"] = np.where(df_s[state]!=df_s[state].shift(1),True,False)
            # Add in last part to get last window
            df_s[f"State_{state}_switchpoint"] = np.where(df_s["window"]==len(df_s),True,df_s[f"State_{state}_switchpoint"])
            df_t = df_s[["window",state,f"State_{state}_switchpoint"]]
            df_t = df_t[df_t[f"State_{state}_switchpoint"]==True]
            df_t["state_model"] = state
            df_t["count"] = df_t[state]
            df_t = df_t[["window","state_model","count"]]
            df_switch = pd.concat([df_switch,df_t])


        df_windowlengths = self.df[["start","end","window","strand"]]
        df_windowlengths = df_windowlengths.drop_duplicates(subset=["window"])
        df_switch = df_switch.merge(df_windowlengths,how="left",on="window")


#         print(df_windowlengths)
#         print(df_switch)
        df_switch["latent_state"] = df_switch.groupby("state_model").cumcount()+1
        df_switch.to_csv(f'{self.name}_MultiStates_Switchpoints.tsv',sep="\t",index=None)

        df_multistates = df_switch.merge(df_ms,how="left",on="state_model")
        df_multistates["name"] = self.name
        df_multistates["fname"] = self.fname
        df_multistates = df_multistates[["fname","name","strand","state_model","latent_state","neg_loss","window","start","end","count"]]
        df_multistates["count"] = df_multistates["count"].astype(float).round(decimals=3)

        self.df_multistates = df_multistates
        self.df_best_model =  df_multistates[df_multistates["state_model"]==self.best_model]

        fout = f'{self.name}_DF.tsv'

        self.df_best_model = self.df_best_model[["fname","window","name","start","end","count"]]

        if path.exists(fout):
            self.df.to_csv(fout,sep="\t",index=None,mode="a",header=None)
            df_multistates.to_csv(f'{self.name}_AllModels.tsv',sep="\t",index=None,mode="a",header=None)
            self.df_best_model.to_csv(f'{self.name}_BestModel.tsv',sep="\t",index=None,mode="a",header=None)
        else:
            self.df.to_csv(fout,sep="\t",index=None,mode="w")
            df_multistates.to_csv(f'{self.name}_AllModels.tsv',sep="\t",index=None,mode="w")
            self.df_best_model.to_csv(f'{self.name}_BestModel.tsv',sep="\t",index=None,mode="w")

        fig = plt.figure(figsize=(14, 12))
        for i, learned_model_rates in enumerate(rates):
          best = ""
          state = i + 1
          ax = fig.add_subplot(4, 3, state)
          if state == self.best_model:
            color = "green"
            best = "(best model)"
          else:
            color="red"
          ax.plot(learned_model_rates[most_probable_states[i]], c=color, lw=3, label='inferred rate')
          ax.plot(observed_counts, c='black', alpha=0.3, label='observed counts')
          ax.set_ylabel("latent rate")
          ax.set_xlabel("windows")
          ax.set_title(f"state_model_{state} {best}")
          ax.legend(loc="upper right")
        plt.tight_layout()
        plt.savefig(f'{self.name}_MultiStates_{self.fname}.png', bbox_inches='tight',transparent=False)
        plt.close()



def ClusterSwitchpoints(df_input,minDistanceWindow=4):
    names = df_input["name"].unique()
    df_final = pd.DataFrame()
    for i in range(len(names)):
        name = names[i]
        print(f"Clustering {name} |{i+1} of {len(names)}")


        ### Add histogram
        df = pd.read_csv(f'{name}_DF.tsv',sep="\t")
        max_window = df["window"].max()
        fnames = df["fname"].unique()
        if len(fnames) == 2:
            palette = ["black","red"]
        if len(fnames) == 1:
            palette = ["black"]

        df2 = df_input[df_input["name"]==name]
        df2 = df2.drop_duplicates(subset=["window"])
        df2 = df2.sort_values(by=["window"], ascending=True)
        z = df2[["count","window"]].to_numpy()
#         z = df2[["window"]].to_numpy()
        X = StandardScaler().fit_transform(z)

        # Compute DBSCAN
        db = DBSCAN(eps=0.3, min_samples=1).fit(X)
        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
        core_samples_mask[db.core_sample_indices_] = True
        labels = db.labels_

        # Number of clusters in labels, ignoring noise if present.
        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise_ = list(labels).count(-1)

#         print("Silhouette Coefficient: %0.3f"% metrics.silhouette_score(X, labels))
        df2["cluster_label"] = labels
        df2["cluster_label"] = df2["cluster_label"] + 1

        plt=sns.scatterplot(data=df2, x="window", y="count", hue="cluster_label",palette=("tab10"),style="fname",s=500)
        sns.lineplot(x="window", y="count", data=df,hue="fname",palette=palette,alpha=0.3)
        plt.figure.savefig(f'{name}_Switchpoints_AllS.png')
        plt.figure.clf()
        df2 = df2.drop_duplicates(subset=["window"])
        df2['window'] = df2['window'].groupby(df2['cluster_label']).transform('mean')
        df2 = df2.drop_duplicates(subset=["cluster_label"])
        plt=sns.scatterplot(data=df2, x="window", y="count", hue="cluster_label",palette=("tab10"),style="fname",s=500)
        sns.lineplot(x="window", y="count", data=df,hue="fname",palette=palette,alpha=0.3)
        plt.figure.savefig(f'{name}_Switchpoints_DBScan_WindowCount.png')
        plt.figure.clf()

        z = df2[["window"]].to_numpy()
        X = StandardScaler().fit_transform(z)
        # Compute DBSCAN
        db = DBSCAN(eps=0.1, min_samples=1).fit(X)
        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
        core_samples_mask[db.core_sample_indices_] = True
        labels = db.labels_

        # Number of clusters in labels, ignoring noise if present.
#         n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
#         n_noise_ = list(labels).count(-1)

#         print('Estimated number of clusters: %d' % n_clusters_)
#         print('Estimated number of noise points: %d' % n_noise_)
#         print("Silhouette Coefficient: %0.3f"% metrics.silhouette_score(X, labels))
        df2["cluster_label"] = labels
        df2["cluster_label"] = df2["cluster_label"] + 1

        df2['window'] = df2['window'].groupby(df2['cluster_label']).transform('mean')
#         df2['switch_start'] = df2['start'].groupby(df2['cluster_label']).transform('mean').astype(int)
#         df2['switch_end'] = df2['end'].groupby(df2['cluster_label']).transform('mean').astype(int)
        df2 = df2.drop_duplicates(subset=["cluster_label"])
        df2 = df2.sort_values(by=["window"], ascending=True)
        df2.iloc[0, df2.columns.get_loc('window')] = 1
        df2.iloc[-1, df2.columns.get_loc('window')] = max_window

        df2 = df2.sort_values(by=["window"], ascending=True)
        df2 = df2[df2["window"]!=1]
        df2 = df2.reset_index(drop=True)
        df2["latent_state"] = df2.index + 1
        df2["latent_state"] = df2["latent_state"].astype(int)

        df2["first_last_state"] = "middle"


        if len(df2) == 1:
            df2["first_last_state"] = "single"
#             df2.iloc[0, df2.columns.get_loc('first_last_state')] = "single"

        if len(df2) > 1:
            df2.iloc[0, df2.columns.get_loc('first_last_state')] = "start"
            df2.iloc[-1, df2.columns.get_loc('first_last_state')] = "end"
#             df2["first_last_state"] = "double"
#             df2.iloc[0, df2.columns.get_loc('first_last_state')] = "single"

        df2.to_csv(f'{name}_FinalSwitchpoints.tsv',sep="\t",index=None)

        df_final = pd.concat([df_final,df2])
        df2["count"] = 0
        plt = sns.scatterplot(data=df2, x="window", y="count", hue="latent_state",palette=("tab10"),s=500)
        sns.lineplot(x="window", y="count", data=df,hue="fname",palette=palette,alpha=0.3)
        plt.figure.savefig(f'{name}_SwitchpointsFinal.png')
        plt.figure.clf()
    return df_final



def makeSwitchpointGTF(df_sp,df,SwitchTitle,reads_window = 50):
    df_sp = df_sp[["window","name","latent_state","first_last_state"]]
    df_sp["name_latent_state"] = df_sp["name"] + "_" + df_sp["latent_state"].astype(str)


     # Keep longest DoGs
    df_p = df[df["strand"]=="+"].copy()
    df_p = df_p.sort_values(by=["DOG_end_local"], ascending=True)
    df_p = df_p.drop_duplicates(subset="name",keep="last")
    df_m = df[df["strand"]=="-"].copy()
    df_m = df_m.sort_values(by=["DOG_start_local"], ascending=True)
    df_m = df_m.drop_duplicates(subset="name",keep="first")
    df = pd.concat([df_p,df_m])

    # Add in genes for later
    df_gene = df.copy()
    df_gene["type"] = "gene"
    df_gene["latent_state"] = 0
    df_gene["name_latent_state"] = df_gene["name"] + "_gene"

    df = df_sp.merge(df, on="name",how="left")
    df["bp"] = (df["window"]-1) * reads_window
    df["bp"] = df["bp"].astype(int)

    df_fin = pd.DataFrame()
    df["latent_state"] = df["latent_state"].astype(int)

    df_p = df[df["strand"]=="+"].copy()
    df_p["switch_end"] = df_p["end"] + df_p["bp"]
    df_p["switch_start"] = df_p["switch_end"].shift(1) + 1

    df_p["switch_start"] = np.where(df_p["first_last_state"]=="start",df_p["DOG_start_local"],df_p["switch_start"])
    df_p["switch_start"] = np.where(df_p["first_last_state"]=="single",df_p["DOG_start_local"],df_p["switch_start"])
    df_p["switch_end"] = np.where(df_p["first_last_state"]=="end",df_p["DOG_end_local"],df_p["switch_end"])
    df_p["switch_end"] = np.where(df_p["first_last_state"]=="single",df_p["DOG_end_local"],df_p["switch_end"])

    df_m = df[df["strand"]=="-"].copy()
    df_m["switch_start"] = df_m["start"] - df_m["bp"]
    df_m["switch_end"] = df_m["switch_start"].shift(1) - 1

    df_m["switch_end"] = np.where(df_m["first_last_state"]=="start",df_m["DOG_end_local"],df_m["switch_end"])
    df_m["switch_end"] = np.where(df_m["first_last_state"]=="single",df_m["DOG_end_local"],df_m["switch_end"])
    df_m["switch_start"] = np.where(df_m["first_last_state"]=="end",df_m["DOG_start_local"],df_m["switch_start"])
    df_m["switch_start"] = np.where(df_m["first_last_state"]=="single",df_m["DOG_start_local"],df_m["switch_start"])

#     df = df_m
    df = pd.concat([df_p,df_m])
    df["start"] = df["switch_start"].astype(int)
    df["end"] = df["switch_end"].astype(int)
    df["type"] = "latent_state"

    df = pd.concat([df,df_gene])

    # Write out gtf
    df["gene_id"] = 'gene_id \"' + df["name_latent_state"]+ '\";'
    df["source"] = "NA"
    df["dot"] = "."
    df["dot2"] = "."


    df["type"] = "gene"
    df_gtf = df[["chr","source","type","start", "end","dot","strand","dot2","gene_id"]]

    #     makeFolders([f"{SwitchOutFolder}/gtf",f"{SwitchOutFolder}/bed"])
    gtf_out = f"{chrom}_{SwitchTitle}.gtf"
    df_gtf.to_csv(gtf_out, sep="\t", index=None, header=None,   quoting=csv.QUOTE_NONE)


    bed_out = f"{SwitchTitle}.bed"
    purple = "157,50,238"
    light_green = "144,238,144"
    Lime = "0,255,0"
    Blue = "0,0,255"
    Yellow = "255,255,0"
    Cyan = "0,255,255"
    Magenta = "255,0,255"
    Maroon = "128,0,0"

    df["RGB"] = Blue
    df["RGB"] = np.where(df["latent_state"]==1,Lime,df["RGB"])
    df["RGB"] = np.where(df["latent_state"]==2,purple,df["RGB"])
    df["RGB"] = np.where(df["latent_state"]==3,Yellow,df["RGB"])
    df["RGB"] = np.where(df["latent_state"]==4,Cyan,df["RGB"])
    df["RGB"] = np.where(df["latent_state"]==5,Magenta,df["RGB"])
    df["RGB"] = np.where(df["latent_state"]==6,Maroon,df["RGB"])
    df["start2"] = df["start"]
    df["stop2"] = df["end"]
    df["zero"] = "0"

    df_bed = df[["chr","start","end","name_latent_state","zero","strand","start2","stop2","RGB"]]
    open(bed_out, 'w').write('track name="'+SwitchTitle+'" description="'+SwitchTitle+'" visibility=2 itemRgb="On"\n')
    df_bed.to_csv(bed_out, sep="\t", index=None, columns=None, header=None)

    del df["zero"]
    del df["dot"]
    del df["dot2"]
    del df["start2"]
    del df["stop2"]
    del df["source"]
    del df["gene_id"]
    del df["type"]
#     df = df[["name","name_latent_state","strand","start","end","first_last_state","switch_start","switch_end","TYPE"]]
    return df, gtf_out


# reads_window = 50
# df_f, gtf_out = makeSwitchpointGTF(df_sp,OutFolder,SwitchTitle)


# df_m = df_f[df_f["strand"]=="-"]
# df_m
# list(df_f)

def get_sig_csv(df, padj, log2FoldChange):
    df["significant"] = np.where(df["padj"] < padj, True, False)
    df_final = df[df.significant]  #Keep all True values. (significant)
#     del df_final["significant"]
    df_final = df_final.copy()
    #Treatment vs Control

    df_final["T_vs_C_up"] = np.where(df_final["log2FoldChange"] > log2FoldChange, True, False)
    df_T_vs_C_up = df_final[df_final.T_vs_C_up]  #Keep all True values. (up)
    del df_T_vs_C_up["T_vs_C_up"]

    df_final["T_vs_C_down"] = np.where(df_final["log2FoldChange"] < log2FoldChange, True, False)
    df_T_vs_C_down = df_final[df_final.T_vs_C_down]  #Keep all True values. (down)
    del df_T_vs_C_down["T_vs_C_up"]
    del df_T_vs_C_down["T_vs_C_down"]
    return df_T_vs_C_up, df_T_vs_C_down


# f_dseq = f"{SwitchOutFolder}/initial_Rsubread_DESeq2/DESeq2_sense.csv"
# padj = 0.05
# log2FoldChange = 0

# df_g = pd.read_csv(f_dseq, sep=",")
# df_g.rename(columns={"Unnamed: 0" : "name_latent_state"}, inplace=True) #Rename unnamed column of deseq2
# df_up, df_do = get_sig_csv(df_g, padj, log2FoldChange)
# df_up["significant_condition"] = "T"
# df_do["significant_condition"] = "C"
# df_sig = pd.concat([df_up,df_do])
# df_all = pd.merge(df_f,df_sig,how="outer",on="name_latent_state")

# df_latent = df_all[~df_all['name_latent_state'].str.contains(r'_gene')]
# df_gene = df_all[df_all['name_latent_state'].str.contains(r'_gene')]
# # df_cx = df_g[df_g["name_latent_state"].str.contains(r'CXXC4')]


def makeSigBed(df,bedtitle,SwitchOutFolder,SwitchTitle):
    df = df[df["significant"]==True]
    df["start2"] = df["start"]
    df["stop2"] = df["end"]
    df["zero"] = "0"
    df = df[["chr","start","end","name_latent_state","zero","strand","start2","stop2","RGB"]]

    makeFolders([f"{SwitchOutFolder}/Significant/"])
    bed_out = f"{SwitchOutFolder}/Significant/{SwitchTitle}_{bedtitle}.bed"
    open(bed_out, 'w').write('track name="'+SwitchTitle+'" description="'+SwitchTitle+'" visibility=2 itemRgb="On"\n')

    df.to_csv(bed_out, sep="\t", index=None, columns=None, header=None)

