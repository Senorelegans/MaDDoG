{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:01.888345Z",
     "start_time": "2019-08-16T20:51:55.737794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:06.222596Z",
     "start_time": "2019-08-16T20:52:01.890515Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_00 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:06.257940Z",
     "start_time": "2019-08-16T20:52:06.224658Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def parseMatchFamClass(line):\n",
    "        line = ' '.join(line.split()) +\"\\n\"\n",
    "        L = line.split(\" \")\n",
    "        Lstart = L[:10]\n",
    "        Lend = L[11:]\n",
    "        class_fam = L[10]\n",
    "        match = L[9]\n",
    "        if \"/\" not in class_fam:\n",
    "            class_fam = [f'{class_fam}',f'{class_fam}']\n",
    "        else:\n",
    "            class_fam = class_fam.split(\"/\")\n",
    "        cla = class_fam[0]\n",
    "        family = class_fam[1]\n",
    "        name = [f'{match}|{family}|{cla}']\n",
    "        \n",
    "        L = Lstart+ name+class_fam+Lend\n",
    "        line = \"\\t\".join(L)\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:06.339903Z",
     "start_time": "2019-08-16T20:52:06.261865Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def writeFiltered(line, removelist,outfile2,outfile3):\n",
    "    WRITE = True\n",
    "    for r in removelist:\n",
    "        if r in line:\n",
    "            WRITE=False\n",
    "            outfile3.write(line)\n",
    "    if WRITE:\n",
    "        outfile2.write(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:06.400390Z",
     "start_time": "2019-08-16T20:52:06.346318Z"
    },
    "code_folding": [
     39
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def flankFasta(df,num):\n",
    "    df = df[df[\"start\"]>num]\n",
    "    df[\"L_fa\"] = df[\"extrafasta\"].str[0:num]\n",
    "    df[\"R_fa\"] = df[\"extrafasta\"].str[-num:]\n",
    "    df[\"fa\"] = df[\"extrafasta\"].str[num:-num]\n",
    "    return df\n",
    "\n",
    "def makeFasta(df,fout,fa):\n",
    "    df2 = pd.DataFrame()\n",
    "    df2[\"data\"] = \">\"+df[\"data\"]\n",
    "    df2[fa] = df[fa]\n",
    "    df2.to_csv(f'{fout}.fa',sep=\"\\n\",index=None,header=False)\n",
    "#     removeAndZip(f\"gzip {fout}_flank_{fa}.fa\")\n",
    "\n",
    "def parseFasta(f, folderout):\n",
    "    f = Path(f)\n",
    "    fout = f'{folderout}/{f.stem}'\n",
    "    \n",
    "    df = pd.read_csv(f,sep=\"\\t\",names=[\"data\",\"fasta\"])\n",
    "    df2 = df[\"data\"].str.rsplit(\";\", expand=True)\n",
    "    D = df2[0].str.rsplit(\"|\", expand=True)\n",
    "    df[\"chr\"] = D[0].str.rsplit(\"=\", expand=True)[1]\n",
    "    df[\"start\"] = D[1].astype(int)\n",
    "    df[\"stop\"] = D[2].astype(int)\n",
    "#     D = df2[1].str.rsplit(\"=\", expand=True)[1].str.rsplit(\"|\",expand=True)\n",
    "    print(df2.head())\n",
    "    df[\"name\"] = df2[1].str.rsplit(\"=\", expand=True)[1]\n",
    "    D3 = df[\"name\"].str.rsplit(\"|\", expand=True)\n",
    "    df[\"MR\"] = D3[0]\n",
    "    df[\"FAMILY\"] = D3[1]\n",
    "    df[\"CLASS\"] = D3[2]\n",
    "    df[\"strand\"] = df2[2].str.rsplit(\"=\", expand=True)[1].str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "#     df = flankFasta(df,10000)\n",
    "    makeFasta(df,fout,fa=\"fasta\")\n",
    "    \n",
    "    df= df[[\"chr\",\"start\",\"stop\",\"name\",\"MR\",\"FAMILY\",\"CLASS\",\"strand\",\"fasta\"]]\n",
    "    df.to_csv(fout+\".txt\",sep=\"\\t\",index=None)\n",
    "        \n",
    "#     removeAndZip(f'{fout}_.txt')\n",
    "\n",
    "def parseFastaExtra(f,num, folderout):\n",
    "    f = Path(f)\n",
    "    fout = f'{folderout}/{f.stem}'\n",
    "    \n",
    "    df = pd.read_csv(f,sep=\"\\t\",names=[\"data\",\"extrafasta\"])\n",
    "    df2 = df[\"data\"].str.rsplit(\";\", expand=True)\n",
    "    D = df2[0].str.rsplit(\"|\", expand=True)\n",
    "    df[\"chr\"] = D[0].str.rsplit(\"=\", expand=True)[1]\n",
    "    df[\"start\"] = D[1].astype(int)\n",
    "    df[\"stop\"] = D[2].astype(int)\n",
    "    D = df2[1].str.rsplit(\"=\", expand=True)[1].str.rsplit(\"|\",expand=True)\n",
    "    df[\"wstart\"] = D[0].astype(int)\n",
    "    df[\"wstop\"] = D[1].astype(int)\n",
    "    \n",
    "    df[\"name\"] = df2[3].str.rsplit(\"=\", expand=True)[1]\n",
    "    D3 = df[\"name\"].str.rsplit(\"|\", expand=True)\n",
    "    df[\"MR\"] = D3[0]\n",
    "    df[\"FAMILY\"] = D3[1]\n",
    "    df[\"CLASS\"] = D3[2]\n",
    "    df[\"strand\"] = df2[4].str.rsplit(\"=\", expand=True)[1].str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "    df = flankFasta(df,10000)\n",
    "    del df[\"extrafasta\"]\n",
    "    makeFasta(df,fout,fa=\"fa\")\n",
    "    makeFasta(df,fout,fa=\"L_fa\")\n",
    "    makeFasta(df,fout,fa=\"R_fa\")\n",
    "    df= df[[\"chr\",\"start\",\"stop\",\"wstart\",\"wstop\",\"name\",\"MR\",\"FAMILY\",\"CLASS\",\"strand\",\"fa\",\"L_fa\",\"R_fa\"]]\n",
    "    df.to_csv(fout+\"_flank.txt\",sep=\"\\t\",index=None)\n",
    "    removeAndZip(f'{fout}_flank.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:30.757682Z",
     "start_time": "2019-08-16T20:52:30.629282Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def ParseRM(f,path,genome,removelist,extension=\".fa.out\",JUPYTER=False):\n",
    "    \"\"\"\n",
    "    path: this will be the path to Repeatmasker output. \n",
    "    It will search for any files in this path or folders in thie path that have the extension.\n",
    "    genome: fasta to get repetitive sequences from\n",
    "    removelist: comma seperated list of things to remove\n",
    "    JUPYTER:set to True to run test examples\n",
    "    \n",
    "    \"\"\"\n",
    "    # If running in jupyter write to RM path. Else do it in nextflow\n",
    "    if JUPYTER:\n",
    "        folderout = f'{path}/ParsedRepeatMaskerGFF'\n",
    "        makeFolders([folderout])\n",
    "    else:\n",
    "        folderout = \".\"\n",
    "        \n",
    "\n",
    "    genomename = Path(f).stem[:-3] # Remove extra .fa\n",
    "    outname = f'{folderout}/{genomename}'\n",
    "    \n",
    "\n",
    "    rlist = \"-\".join(removelist)\n",
    "    fout = f'{outname}.out'\n",
    "    open(fout, \"w\").write(\"\")\n",
    "    foutRNA = f'{outname}_cleaned-{rlist}.out'\n",
    "    open(foutRNA, \"w\").write(\"\")\n",
    "    foutrem = f'{outname}_garbage-{rlist}.out'\n",
    "    open(foutrem, \"w\").write(\"\")\n",
    "    with open(fout,\"a\") as outfile1, open(foutRNA,\"a\") as outfile2, open(foutrem,\"a\") as outfile3:\n",
    "        header = \"bit score\\tperc div\\tperc del\\tperc ins\\tchr\\tstart\\tend\\tpoqleft\\tstrand\\tmatching_repeat\\tname\\tclass\\tfamily\\tpirbegin\\tpirend\\tpirleft\\tID\\n\"\n",
    "        outfile1.write(header),outfile2.write(header),outfile3.write(header)\n",
    "        with open(f,\"r\") as infile:\n",
    "            x = 0\n",
    "            for line in infile:\n",
    "                if x > 2:\n",
    "                    line = parseMatchFamClass(line)\n",
    "                    line = line.replace(\"\\t*\",\"\")\n",
    "                    writeFiltered(line,removelist,outfile2, outfile3)\n",
    "                    outfile1.write(line)\n",
    "                x += 1\n",
    "    removeAndZip(fout)  \n",
    "    removeAndZip(foutrem)\n",
    "    ##### Make bed files ####################\n",
    "    extralength = 10000\n",
    "    outlength = extralength / 1000\n",
    "    files = [fout,foutRNA,foutrem]\n",
    "    files= [foutRNA]\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,sep=\"\\t\")\n",
    "        removeFile(f)\n",
    "        df[\"strand\"] = df[\"strand\"].replace(\"C\",\"-\")\n",
    "        df[\"dot\"] = \".\"\n",
    "        df[\"attribute\"] = \"loc=\"+df[\"chr\"]+\"|\"+df[\"start\"].astype(str)+\"|\"+df[\"end\"].astype(str)+\"|ID=\"+df[\"ID\"].astype(str)+\";name=\"+df[\"name\"]+\";strand=\"\n",
    "        df[\"attribute2\"] = \"loc=\"+df[\"chr\"]+\"|\"+df[\"start\"].astype(str)+\"|\"+df[\"end\"].astype(str)+\"|ID=\"+df[\"ID\"].astype(str)+\";name=\"+df[\"name\"]+\";strand=\"+df[\"strand\"]+\";\"\n",
    "        df[\"attribute3\"] = \"loc=\"+df[\"chr\"]+\"|\"+df[\"start\"].astype(str)+\"|\"+df[\"end\"].astype(str)+\"|ID=\"+df[\"ID\"].astype(str)+\";name=\"+df[\"name\"]+\";strand=(\"+df[\"strand\"]+\")\"\n",
    "        df1 = df[[\"chr\",\"dot\",\"attribute\",\"start\",\"end\",\"dot\",\"strand\",\"dot\",\"attribute2\"]]\n",
    "        df1.to_csv(f'{f[:-4]}.gff',sep=\"\\t\",index=None,header=None)\n",
    "        subprocess.run(f\"bedtools getfasta -tab -name -s -fi {genome} -bed {f'{f[:-4]}.gff'} > {f'{f[:-4]}.txt'}\", shell=True)\n",
    "        parseFasta(f=f'{f[:-4]}.txt', folderout=folderout)\n",
    "        df2 = df[[\"chr\",\"dot\",\"attribute3\",\"start\",\"end\",\"dot\",\"strand\",\"dot\",\"attribute2\"]]\n",
    "        df2.to_csv(f'{f[:-4]}.gff',sep=\"\\t\",index=None,header=None)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         # Add extra length to sequences\n",
    "#         df[\"newstart\"] = df[\"start\"] - extralength\n",
    "#         df[\"newstart\"] = np.where(df[\"newstart\"]<1,1,df[\"newstart\"])\n",
    "#         df[\"newend\"] = df[\"end\"] + extralength\n",
    "#         df[\"attribute\"] = \"loc=\"+df[\"chr\"]+\"|\"+df[\"start\"].astype(str)+\"|\"+df[\"end\"].astype(str)+\";wideloc=\"+df[\"newstart\"].astype(str)+\"|\"+df[\"newend\"].astype(str)+\";ID=\"+df[\"ID\"].astype(str)+\";name=\"+df[\"name\"]+\";strand=\"\n",
    "#         df = df[[\"chr\",\"dot\",\"attribute\",\"newstart\",\"newend\",\"dot\",\"strand\",\"dot\",\"attribute\"]]\n",
    "        \n",
    "#         f_extra = f'{f[:-4]}_+-{outlength}kb.gff'\n",
    "#         f_extra_out = f'{f[:-4]}_+-{outlength}kb.txt'\n",
    "#         df.to_csv(f_extra,sep=\"\\t\",index=None,header=None)\n",
    "        \n",
    "#         subprocess.run(f\"bedtools getfasta -tab -name -s -fi {genome} -bed {f_extra} > {f_extra_out}\", shell=True)\n",
    "#         removeAndZip(f_extra)\n",
    "#         subprocess.run(f\"gzip {f_extra}\", shell=True)\n",
    "#         parseFasta(f=f_extra_out, folderout=folderout,num=10000)\n",
    "#         removeAndZip(f_extra_out)\n",
    "#         subprocess.run(f\"gzip {f_extra_out}\", shell=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:31.317057Z",
     "start_time": "2019-08-16T20:52:31.211323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           0                         1           2\n",
      "0  loc=chrX|10042|10428|ID=2      name=LTR12E|ERV1|LTR  strand=(+)\n",
      "1  loc=chrX|11800|12270|ID=3       name=LTR6B|ERV1|LTR  strand=(+)\n",
      "2  loc=chrX|12291|12704|ID=4       name=LTR2C|ERV1|LTR  strand=(+)\n",
      "3  loc=chrX|13017|13271|ID=7       name=AluSg|Alu|SINE  strand=(-)\n",
      "4  loc=chrX|14909|15164|ID=8  name=MLT1A|ERVL-MaLR|LTR  strand=(-)\n"
     ]
    }
   ],
   "source": [
    "ParseRM(f=\"test/RepeatMasker_test/chrX.fa.out\",\n",
    "        path=\"test/RepeatMasker_test\",\n",
    "        extension=\".fa.out\",\n",
    "        genome=\"/Users/m/Google_Drive/Scripts/IGVfiles/hg38/hg38.fa\", \n",
    "        removelist = [\"Simple_repeat\",\"Low_complexity\",\"rRNA\"], \n",
    "        JUPYTER=True)\n",
    "\n",
    "\n",
    "# ParseRM(f=\"test/RepeatMasker_test/chrY.fa.out\",\n",
    "#         path=\"test/RepeatMasker_test\",\n",
    "#         extension=\".fa.out\",\n",
    "#         genome=\"/Users/m/Google_Drive/Scripts/IGVfiles/hg38/hg38.fa\", \n",
    "#         removelist = [\"Simple_repeat\",\"Low_complexity\",\"rRNA\"], \n",
    "#         JUPYTER=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:32.266663Z",
     "start_time": "2019-08-16T20:52:32.235451Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import argparse\n",
    "def parse_arguments():\n",
    "        parser = argparse.ArgumentParser(description='Checking Paired or not or if its zipped.')\n",
    "        parser.add_argument('--file', action= 'store', metavar='file') \n",
    "        parser.add_argument('--path', action= 'store', metavar='path') \n",
    "        parser.add_argument('--genome', action= 'store', metavar='genome')\n",
    "        parser.add_argument('--verbose', action= 'store', metavar='verbose', default= False) \n",
    "        parser.add_argument('--removelist', action= 'store', metavar='removelist', default= False) \n",
    "        parser.add_argument('--JUPYTER', action= 'store', metavar='JUPYTER', default= False)\n",
    "        args = parser.parse_args()\n",
    "        return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:32.892381Z",
     "start_time": "2019-08-16T20:52:32.859250Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--file file] [--path path]\n",
      "                             [--genome genome] [--verbose verbose]\n",
      "                             [--removelist removelist] [--JUPYTER JUPYTER]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/M/Library/Jupyter/runtime/kernel-867a1d04-67e2-4969-abd3-d82a7cb9adc8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/M/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "#Ignore error, run this cell to get proper export of function\n",
    "#to python script\n",
    "if __name__==\"__main__\":\n",
    "    args = parse_arguments()\n",
    "    verbose = fix_bool(args.verbose)\n",
    "    JUPYTER = fix_bool(args.JUPYTER)\n",
    "    f = fix_bool(args.file)\n",
    "    path = args.path\n",
    "    genome = args.genome\n",
    "    removelist = args.removelist.split(\",\")\n",
    "    ParseRM(f=f,path=path, removelist = removelist, genome=genome, JUPYTER=JUPYTER)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T20:52:36.425540Z",
     "start_time": "2019-08-16T20:52:36.189390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01.1ParseRepeatMaskerOutput.ipynb to nb_01.1ParseRepeatMaskerOutput.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 01.1ParseRepeatMaskerOutput.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T15:05:42.810085Z",
     "start_time": "2019-07-31T15:05:42.768092Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test python script in bash\n",
    "#!python nb_01ParseRepeatMaskerOutput.py --path \"test/RepeatMasker_test\" --removelist \"Simple_repeat,Low_complexity,rRNA\" --genomename  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
